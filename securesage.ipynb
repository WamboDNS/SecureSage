{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "612c90e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_PATH = \"./test/code1.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e1f06b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import Optional, Dict, Any\n",
    "from openai import OpenAI\n",
    "import tempfile\n",
    "import subprocess\n",
    "import json\n",
    "import ast\n",
    "import requests\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "BRAVE_API_KEY = os.getenv(\"BRAVE_API_KEY\", \"\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable is not set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dd6140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4.1\"\n",
    "base_URL = \"https://api.openai.com/v1\"\n",
    "client = OpenAI(api_key=OPENAI_API_KEY, base_url=base_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4655cc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are SecureSage â€” a vigilant, intelligent, and explainable security analyst. Your task is to review Python source code files, understand what the code is doing, and identify potential security vulnerabilities, such as insecure deserialization, command injection, hardcoded secrets, and other OWASP Top 10 issues.\n",
    "\n",
    "You do this by performing step-by-step analysis. You are allowed to use the following tools:\n",
    "\n",
    "- load_code(path: str) -> str: Loads and returns the contents of a Python (.py) source code file.\n",
    "- static_analysis(code: str) -> str: Runs static security scanners (e.g., Bandit) and returns a list of flagged lines with issue types and severity.\n",
    "- parse_ast(code: str) -> str: Parses the code into an abstract syntax tree and extracts function names, inputs, and risky constructs (e.g., eval, exec, os.system).\n",
    "- doc_search_with_brave(query: str) -> str: Performs a live search using the Brave Search API to retrieve recent documentation and best practices from sources like OWASP, CWE, and security blogs. The results are summarized using your reasoning ability. Use this tool when you need external context or to validate the risk or mitigation of a specific pattern.\n",
    "- suggest_fix(issue: str, code_snippet: str) -> str: Proposes a secure version of the code snippet that mitigates the vulnerability.\n",
    "\n",
    "You may call one tool per turn, for up to 10 turns, before giving your final answer.\n",
    "\n",
    "In each turn, respond in the following format:\n",
    "\n",
    "<think>\n",
    "[Explain what you're doing next, what you need, or what issue you're focusing on.]\n",
    "</think>\n",
    "<tool>\n",
    "JSON with the following fields:\n",
    "- name: The name of the tool to call\n",
    "- args: A dictionary of arguments to pass to the tool (must be valid JSON)\n",
    "</tool>\n",
    "\n",
    "When you are done, provide a clear and structured security review in the following format:\n",
    "\n",
    "<answer>\n",
    "1. Summary of Code Purpose  \n",
    "2. Detected Vulnerabilities (with line numbers and severity) and explanation of each issue (why it's dangerous, relevant CVE/CWE/OWASP ref)  \n",
    "3. Suggested Fixes (with example code and links if needed)  \n",
    "</answer>\n",
    "\n",
    "The answer should be well-structured and easily readable.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7f28c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_code(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def static_analysis(code: str) -> list:\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".py\", mode='w+', delete=False) as tmp:\n",
    "        tmp.write(code)\n",
    "        tmp.flush()\n",
    "        result = subprocess.run(\n",
    "            [\"bandit\", \"-f\", \"json\", tmp.name],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        try:\n",
    "            output = json.loads(result.stdout)\n",
    "            return [\n",
    "                {\n",
    "                    \"line\": item[\"line_number\"],\n",
    "                    \"issue\": item[\"issue_text\"],\n",
    "                    \"severity\": item[\"issue_severity\"],\n",
    "                    \"confidence\": item[\"issue_confidence\"],\n",
    "                    \"id\": item[\"test_id\"]\n",
    "                }\n",
    "                for item in output.get(\"results\", [])\n",
    "            ]\n",
    "        except Exception as e:\n",
    "            return [{\"error\": str(e)}]\n",
    "\n",
    "def parse_ast(code: str) -> dict:\n",
    "    tree = ast.parse(code)\n",
    "    functions = []\n",
    "    risky_calls = []\n",
    "    imports = []\n",
    "\n",
    "    class Analyzer(ast.NodeVisitor):\n",
    "        def visit_FunctionDef(self, node):\n",
    "            functions.append(node.name)\n",
    "            self.generic_visit(node)\n",
    "\n",
    "        def visit_Call(self, node):\n",
    "            if isinstance(node.func, ast.Attribute):\n",
    "                func_name = f\"{ast.unparse(node.func.value)}.{node.func.attr}\"\n",
    "                if func_name in [\"os.system\", \"eval\", \"exec\", \"pickle.load\", \"subprocess.Popen\"]:\n",
    "                    risky_calls.append({\n",
    "                        \"line\": node.lineno,\n",
    "                        \"call\": func_name,\n",
    "                        \"arg\": ast.unparse(node.args[0]) if node.args else \"\"\n",
    "                    })\n",
    "            self.generic_visit(node)\n",
    "\n",
    "        def visit_Import(self, node):\n",
    "            for alias in node.names:\n",
    "                imports.append(alias.name)\n",
    "\n",
    "        def visit_ImportFrom(self, node):\n",
    "            imports.append(node.module)\n",
    "\n",
    "    Analyzer().visit(tree)\n",
    "\n",
    "    return {\n",
    "        \"functions\": functions,\n",
    "        \"risky_calls\": risky_calls,\n",
    "        \"imports\": imports\n",
    "    }\n",
    "    \n",
    "def brave_search(query: str) -> list[str]:\n",
    "    url = \"https://api.search.brave.com/res/v1/web/search\"\n",
    "    headers = {\"Accept\": \"application/json\", \"X-Subscription-Token\": BRAVE_API_KEY}\n",
    "    params = {\"q\": query, \"count\": 5, \"freshness\": \"Month\"}\n",
    "\n",
    "    resp = requests.get(url, headers=headers, params=params)\n",
    "    data = resp.json()\n",
    "    results = data.get(\"web\", {}).get(\"results\", [])\n",
    "    return [r.get(\"title\", \"\") + \"\\n\" + r.get(\"description\", \"\") for r in results]\n",
    "\n",
    "def doc_search_with_brave(query: str, model: str = \"gpt-4.1\") -> str:\n",
    "    results = brave_search(query)\n",
    "    context = \"\\n\\n\".join(results)\n",
    "\n",
    "    prompt = (\n",
    "        \"You are a security expert. Answer the following question using the information \"\n",
    "        \"from recent search results:\\n\\n\"\n",
    "        f\"Search results:\\n{context}\\n\\n\"\n",
    "        f\"Question: {query}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def suggest_fix(issue: str, code_snippet: str, model_name: str = \"gpt-4.1\") -> str:\n",
    "    prompt = (\n",
    "        \"You are a secure code advisor.\\n\"\n",
    "        f\"The following code has a security issue: {issue}.\\n\"\n",
    "        \"Suggest a safer version of the code and explain why it's better.\\n\\n\"\n",
    "        f\"Code:\\n{code_snippet}\"\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81206b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_thinking_from_response(response: str) -> Optional[str]:\n",
    "    \"\"\"Extract the <think> block from the LLM response.\"\"\"\n",
    "    match = re.search(r\"<think>(.*?)</think>\", response, re.DOTALL)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "\n",
    "def parse_tool_from_response(response: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Extract the <tool> call as a dictionary from the LLM response.\"\"\"\n",
    "    match = re.search(r\"<tool>(.*?)</tool>\", response, re.DOTALL)\n",
    "    if not match:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(match.group(1))\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON parsing error in <tool>: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_answer_from_response(response: str) -> Optional[str]:\n",
    "    \"\"\"Extract the <answer> block from the LLM response.\"\"\"\n",
    "    match = re.search(r\"<answer>(.*?)</answer>\", response, re.DOTALL)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "\n",
    "def write_answer_to_markdown(answer: str, file_path: str = \"reports/secure_sage_report.md\") -> None:\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)  # Ensure folder exists\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"# SecureSage Security Report\\n\\n\")\n",
    "        f.write(answer.strip())\n",
    "        f.write(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0bb4f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== AGENT TURN 0 ================\n",
      "\n",
      "\n",
      "Agent thought:\n",
      "I need to start by loading the contents of the source code file \"./test/code1.py\" so I can understand what it's doing and begin the analysis.\n",
      "\n",
      "Tool call:\n",
      "Tool: load_code\n",
      "Args: {\n",
      "  \"path\": \"./test/code1.py\"\n",
      "}\n",
      "\n",
      "Tool result:\n",
      "\"import os\\nimport subprocess\\nimport random\\nimport pickle\\nimport flask\\n\\napp = flask.Flask(__name__)\\napp.config['DEBUG'] = True  \\n\\nAPI_KEY = \\\"123456-SECRET-HARDCODED\\\"  \\n\\ndef delete_file(filename):\\n    os.system(\\\"rm -rf \\\" + filename) \\n\\ndef insecure_pickle(data):\\n    return pickle.loads(data)  \\n\\n@app.route('/run', methods=['POST'])\\ndef run_command():\\n    command = flask.request.form['cmd']\\n    output = subprocess.Popen(command, shell=True)  \\n    return \\\"Done\\\"\\n\\ndef get_token():\\n    return random.random() \\n\"\n",
      "=============== AGENT TURN 1 ================\n",
      "\n",
      "\n",
      "Agent thought:\n",
      "This code includes several potential red flags: use of os.system with user input, use of pickle.loads, subprocess.Popen with shell=True, and a hardcoded API key. My next step is to run a static analysis to flag specific lines and issues, which will guide me in detailing each vulnerability.\n",
      "\n",
      "Tool call:\n",
      "Tool: static_analysis\n",
      "Args: {\n",
      "  \"code\": \"import os\\nimport subprocess\\nimport random\\nimport pickle\\nimport flask\\n\\napp = flask.Flask(__name__)\\napp.config['DEBUG'] = True  \\n\\nAPI_KEY = \\\"123456-SECRET-HARDCODED\\\"  \\n\\ndef delete_file(filename):\\n    os.system(\\\"rm -rf \\\" + filename) \\n\\ndef insecure_pickle(data):\\n    return pickle.loads(data)  \\n\\n@app.route('/run', methods=['POST'])\\ndef run_command():\\n    command = flask.request.form['cmd']\\n    output = subprocess.Popen(command, shell=True)  \\n    return \\\"Done\\\"\\n\\ndef get_token():\\n    return random.random() \\n\"\n",
      "}\n",
      "\n",
      "Tool result:\n",
      "[\n",
      "  {\n",
      "    \"line\": 2,\n",
      "    \"issue\": \"Consider possible security implications associated with the subprocess module.\",\n",
      "    \"severity\": \"LOW\",\n",
      "    \"confidence\": \"HIGH\",\n",
      "    \"id\": \"B404\"\n",
      "  },\n",
      "  {\n",
      "    \"line\": 4,\n",
      "    \"issue\": \"Consider possible security implications associated with pickle module.\",\n",
      "    \"severity\": \"LOW\",\n",
      "    \"confidence\": \"HIGH\",\n",
      "    \"id\": \"B403\"\n",
      "  },\n",
      "  {\n",
      "    \"line\": 13,\n",
      "    \"issue\": \"Starting a process with a shell, possible injection detected, security issue.\",\n",
      "    \"severity\": \"HIGH\",\n",
      "    \"confidence\": \"HIGH\",\n",
      "    \"id\": \"B605\"\n",
      "  },\n",
      "  {\n",
      "    \"line\": 16,\n",
      "    \"issue\": \"Pickle and modules that wrap it can be unsafe when used to deserialize untrusted data, possible security issue.\",\n",
      "    \"severity\": \"MEDIUM\",\n",
      "    \"confidence\": \"HIGH\",\n",
      "    \"id\": \"B301\"\n",
      "  },\n",
      "  {\n",
      "    \"line\": 21,\n",
      "    \"issue\": \"subprocess call with shell=True identified, security issue.\",\n",
      "    \"severity\": \"HIGH\",\n",
      "    \"confidence\": \"HIGH\",\n",
      "    \"id\": \"B602\"\n",
      "  },\n",
      "  {\n",
      "    \"line\": 25,\n",
      "    \"issue\": \"Standard pseudo-random generators are not suitable for security/cryptographic purposes.\",\n",
      "    \"severity\": \"LOW\",\n",
      "    \"confidence\": \"HIGH\",\n",
      "    \"id\": \"B311\"\n",
      "  }\n",
      "]\n",
      "=============== AGENT TURN 2 ================\n",
      "\n",
      "\n",
      "Agent thought:\n",
      "The static analysis has flagged several critical lines, especially os.system, subprocess with shell=True, use of pickle for deserialization, and the use of random for token generation. One additional risk, not flagged by Bandit but visible from the code, is the hardcoded API key. I want to confirm all points in the AST, including function arguments that may propagate user input to risky sinks, and the specific use context.\n",
      "\n",
      "Tool call:\n",
      "Tool: parse_ast\n",
      "Args: {\n",
      "  \"code\": \"import os\\nimport subprocess\\nimport random\\nimport pickle\\nimport flask\\n\\napp = flask.Flask(__name__)\\napp.config['DEBUG'] = True  \\n\\nAPI_KEY = \\\"123456-SECRET-HARDCODED\\\"  \\n\\ndef delete_file(filename):\\n    os.system(\\\"rm -rf \\\" + filename) \\n\\ndef insecure_pickle(data):\\n    return pickle.loads(data)  \\n\\n@app.route('/run', methods=['POST'])\\ndef run_command():\\n    command = flask.request.form['cmd']\\n    output = subprocess.Popen(command, shell=True)  \\n    return \\\"Done\\\"\\n\\ndef get_token():\\n    return random.random() \\n\"\n",
      "}\n",
      "\n",
      "Tool result:\n",
      "{\n",
      "  \"functions\": [\n",
      "    \"delete_file\",\n",
      "    \"insecure_pickle\",\n",
      "    \"run_command\",\n",
      "    \"get_token\"\n",
      "  ],\n",
      "  \"risky_calls\": [\n",
      "    {\n",
      "      \"line\": 13,\n",
      "      \"call\": \"os.system\",\n",
      "      \"arg\": \"'rm -rf ' + filename\"\n",
      "    },\n",
      "    {\n",
      "      \"line\": 21,\n",
      "      \"call\": \"subprocess.Popen\",\n",
      "      \"arg\": \"command\"\n",
      "    }\n",
      "  ],\n",
      "  \"imports\": [\n",
      "    \"os\",\n",
      "    \"subprocess\",\n",
      "    \"random\",\n",
      "    \"pickle\",\n",
      "    \"flask\"\n",
      "  ]\n",
      "}\n",
      "=============== AGENT TURN 3 ================\n",
      "\n",
      "\n",
      "Agent thought:\n",
      "Now I have confirmation: the delete_file function directly concatenates user input (filename) into an os.system command, exposing a command injection risk. The run_command function takes input from flask.request.form['cmd'] and passes it directly to subprocess.Popen with shell=True, which is a severe command injection risk. The insecure_pickle function revolves around unsafe deserialization. get_token uses weak randomness, and there's a hardcoded API key in the global scope. Finally, both the static and AST analyses reinforce that these are concrete vulnerabilities. For completeness, Iâ€™ll validate the best mitigation strategies for these patterns from recent OWASP and security docs.\n",
      "\n",
      "Tool call:\n",
      "Tool: doc_search_with_brave\n",
      "Args: {\n",
      "  \"query\": \"Secure alternatives and mitigations for Python os.system command injection, subprocess shell injection, pickle unsafe deserialization, hardcoded secrets, and random for security tokens OWASP CWE\"\n",
      "}\n",
      "\n",
      "Tool result:\n",
      "\"Certainly! Here\\u2019s a concise summary of **secure alternatives and mitigations** for the Python-specific vulnerabilities and bad practices you listed, referencing OWASP and trusted security sources up to 2024:\\n\\n---\\n\\n### 1. **Python os.system Command Injection & subprocess Shell Injection**\\n- **Description:** These occur when untrusted user input is passed unchecked to functions like `os.system()`, `subprocess.call()`, or `subprocess.Popen()` with `shell=True`. Attackers can execute arbitrary OS commands.\\n- **Risks (OWASP Top Ten: A1-Injection / [CWE-78: OS Command Injection](https://cwe.mitre.org/data/definitions/78.html))**\\n- **Secure Alternatives & Mitigations:**\\n  - **Never use `shell=True` or `os.system()` with untrusted input.**\\n  - Use the [`subprocess`](https://docs.python.org/3/library/subprocess.html) module with a list of arguments & `shell=False` (the default). For example:\\n    ```python\\n    import subprocess\\n    # BAD (vulnerable to injection):\\n    subprocess.call(f\\\"nslookup {user_input}\\\", shell=True)\\n    # GOOD (safe):\\n    subprocess.run([\\\"nslookup\\\", user_input], check=True)\\n    ```\\n  - **Validate and sanitize all user inputs (allowlist only valid formats).**\\n  - Never concatenate user input directly into command strings.\\n\\n---\\n\\n### 2. **pickle Unsafe Deserialization**\\n- **Description:** Using Python\\u2019s `pickle` on untrusted input allows attackers to execute arbitrary code on deserialization (see [CWE-502: Deserialization of Untrusted Data](https://cwe.mitre.org/data/definitions/502.html)).\\n- **Secure Alternatives & Mitigations:**\\n  - **Never unpickle data from untrusted sources.**\\n  - Use safe serialization formats like **JSON** with `json.loads()` and `json.dumps()`:\\n    ```python\\n    import json\\n    data = json.loads(user_supplied_string)\\n    ```\\n  - If object serialization is required, use libraries that enforce strict schemas, e.g., `marshmallow`, `pydantic`.\\n  - Disable or tightly control `pickle` usage (e.g., restrict object types).\\n\\n---\\n\\n### 3. **Hardcoded Secrets**\\n- **Description:** Placing secrets (API keys, passwords, cryptographic keys) directly in code is a major risk (see [CWE-798: Use of Hard-coded Credentials](https://cwe.mitre.org/data/definitions/798.html)).\\n- **Secure Alternatives & Mitigations:**\\n  - **Never store secrets in source code.**\\n  - Use environment variables, secret managers (AWS Secrets Manager, HashiCorp Vault, Azure Key Vault, etc.).\\n  - Employ tools like [python-dotenv](https://pypi.org/project/python-dotenv/), and add `.env` to `.gitignore`.\\n  - Scan code for secrets (e.g., use [GitHub\\u2019s secret scanning](https://docs.github.com/en/code-security/secret-scanning/about-secret-scanning)).\\n\\n---\\n\\n### 4. **random for Security Tokens**\\n- **Description:** Python\\u2019s `random` module is **not cryptographically secure** and must not be used for passwords or tokens ([CWE-338: Use of Cryptographically Weak PRNG](https://cwe.mitre.org/data/definitions/338.html)).\\n- **Secure Alternatives & Mitigations:**\\n  - Use the [`secrets`](https://docs.python.org/3/library/secrets.html) module for all security tokens and secrets:\\n    ```python\\n    import secrets\\n    token = secrets.token_urlsafe(32)\\n    ```\\n  - For password hashing, use libraries like `bcrypt` or `argon2`.\\n\\n---\\n\\n## **General Recommendations (per OWASP and CISA)**\\n- **Validate all user inputs**: Use allow-lists and strict types.\\n- **Keep dependencies updated** and scan for vulnerabilities.\\n- **Apply least privilege**: Applications should have minimal OS/file permissions.\\n- **Code Review and Testing**: Static analysis and regular code audits.\\n\\n### **Resources**\\n- [OWASP Command Injection](https://owasp.org/www-community/attacks/Command_Injection)\\n- [OWASP Python Security Project](https://owasp.org/www-project-python-security/)\\n- [CISA Secure by Design Alert (OS Command Injection)](https://www.cisa.gov/news-events/alerts/2023/04/13/secure-design-alert-eliminating-os-command-injection-vulnerabilities)\\n\\n---\\n\\n**Summary Table**:\\n\\n| Issue                         | Unsafe Example                         | Secure Alternative              |\\n|-------------------------------|----------------------------------------|---------------------------------|\\n| os.system/`shell=True`         | `os.system('ls ' + user_input)`        | `subprocess.run(['ls', user_input])` (no shell)|\\n| Pickle deserialization         | `pickle.loads(user_data)`              | `json.loads(user_data)`         |\\n| Hardcoded secrets              | `password = 'secret'`                  | Store in env/secret manager     |\\n| random for security            | `random.randint()` for tokens          | `secrets.token_hex()`           |\\n\\n---\\n\\nLet me know if you need sample code or a secure implementation for your use case!\"\n",
      "=============== AGENT TURN 4 ================\n",
      "\n",
      "ANSWER GIVEN BY AGENT!\n",
      "Report written to markdown.\n"
     ]
    }
   ],
   "source": [
    "# Agent memory\n",
    "messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "tool_call_count = 0\n",
    "max_turns = 10\n",
    "\n",
    "tool_registry = {\n",
    "    \"load_code\": load_code,\n",
    "    \"static_analysis\": static_analysis,\n",
    "    \"parse_ast\": parse_ast,\n",
    "    \"doc_search_with_brave\": doc_search_with_brave,\n",
    "    \"suggest_fix\": suggest_fix,\n",
    "}\n",
    "\n",
    "\n",
    "user_input = f\"Please analyze {CODE_PATH} for vulnerabilities.\"\n",
    "messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "file_name = os.path.basename(CODE_PATH).replace(\".py\", \"\")\n",
    "\n",
    "while tool_call_count < max_turns:\n",
    "    print(f\"=============== AGENT TURN {tool_call_count} ================\\n\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    reply = response.choices[0].message.content\n",
    "    messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "\n",
    "    thought = parse_thinking_from_response(reply)\n",
    "    if thought:\n",
    "        print(\"\\nAgent thought:\")\n",
    "        print(thought)\n",
    "\n",
    "    answer = parse_answer_from_response(reply)\n",
    "    if answer:\n",
    "        print(\"ANSWER GIVEN BY AGENT!\")\n",
    "\n",
    "        write_answer_to_markdown(answer, file_path=f\"./reports/{file_name}_report.md\")\n",
    "        print(\"Report written to markdown.\")\n",
    "        break\n",
    "\n",
    "    tool_call = parse_tool_from_response(reply)\n",
    "    if not tool_call:\n",
    "        print(\"No tool call found. Exiting.\")\n",
    "        break\n",
    "\n",
    "    tool_name = tool_call[\"name\"]\n",
    "    args = tool_call[\"args\"]\n",
    "\n",
    "    print(\"\\nTool call:\")\n",
    "    print(f\"Tool: {tool_name}\")\n",
    "    print(f\"Args: {json.dumps(args, indent=2)}\")\n",
    "\n",
    "    tool_func = tool_registry.get(tool_name)\n",
    "    if not tool_func:\n",
    "        print(f\"Unknown tool: {tool_name}\")\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        result = tool_func(**args)\n",
    "    except Exception as e:\n",
    "        result = {\"error\": str(e)}\n",
    "\n",
    "    print(\"\\nTool result:\")\n",
    "    print(json.dumps(result, indent=2))\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": json.dumps(result, indent=2)})\n",
    "    tool_call_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2195137",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
