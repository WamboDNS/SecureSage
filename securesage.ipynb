{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "612c90e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_PATH = \"./example_code/test_project/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e1f06b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import Optional, Dict, Any, Union\n",
    "from openai import OpenAI\n",
    "import tempfile\n",
    "import subprocess\n",
    "import json\n",
    "import ast\n",
    "import requests\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "BRAVE_API_KEY = os.getenv(\"BRAVE_API_KEY\", \"\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable is not set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dd6140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4.1\"\n",
    "base_URL = \"https://api.openai.com/v1\"\n",
    "client = OpenAI(api_key=OPENAI_API_KEY, base_url=base_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4655cc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = system_prompt = \"\"\"\n",
    "You are SecureSage — a vigilant, intelligent, and explainable security analyst. Your task is to review Python source code files, understand what the code is doing, and identify potential security vulnerabilities, such as insecure deserialization, command injection, hardcoded secrets, and other OWASP Top 10 issues.\n",
    "It is very important that you try to identify all vulnerabilities and not only the most obvious ones. Do not only look for vulnerabilities of the categories mentioned above!\n",
    "\n",
    "You do this by performing step-by-step analysis. You are allowed to use the following tools:\n",
    "\n",
    "- load_files(path: str) -> str: Loads and returns the contents of a Python (.py) source code file or the contents of each python file in a directory (recursive). The format is a list of (file_path, file_content) tuples. If a directory is provided, you should analyze all files and their interactions.\n",
    "- static_analysis(code: str) -> str: Runs static security scanners (e.g., Bandit) and returns a list of flagged lines with issue types and severity.\n",
    "- parse_ast(code: str) -> str: Parses the code into an abstract syntax tree and extracts function names, inputs, and risky constructs (e.g., eval, exec, os.system).\n",
    "- doc_search_with_brave(query: str) -> str: Performs a live search using the Brave Search API to retrieve recent documentation and best practices from sources like OWASP, CWE, and security blogs. The results are summarized using your reasoning ability. Use this tool when you need external context or to validate the risk or mitigation of a specific pattern.\n",
    "- suggest_fix(issue: str, code_snippet: str) -> str: Proposes a secure version of the code snippet that mitigates the vulnerability.\n",
    "\n",
    "You may call one tool per turn, for up to 10 turns, before giving your final answer.\n",
    "\n",
    "When analyzing a directory with multiple files:\n",
    "- First, load all files to get an overview.\n",
    "- Keep track of the files you have analyzed and their key components (e.g., functions, global variables, sensitive data handling, imports/exports).\n",
    "- Pay close attention to how files interact. For example:\n",
    "    - If `file_A.py` defines a function that takes user input or reads sensitive data.\n",
    "    - And `file_B.py` imports and calls this function from `file_A.py`, passing its output to a dangerous sink (like `os.system`, SQL execution, or writing to a web response).\n",
    "    - This constitutes a critical cross-file vulnerability (e.g., command injection, XSS, SQLi, insecure deserialization if the data is pickled/unpickled across files).\n",
    "- Trace data flow across files. Identify sources (e.g., user input, file reads, network requests, configurations) and sinks (e.g., command execution, database queries, file writes, sensitive data exposure, deserialization).\n",
    "- Note any hardcoded secrets, weak configurations (e.g., in `config.py`), or sensitive information defined in one file that are imported and used, potentially insecurely, in another.\n",
    "- Your analysis should be thorough, considering all functions, methods, and the context in which they are used, especially inter-dependencies. Use `parse_ast` to understand imports and function calls between files.\n",
    "\n",
    "In each turn, respond in the following format:\n",
    "\n",
    "<think>\n",
    "[Explain what you're doing next, what you need, or what issue you're focusing on. If analyzing multiple files, explain how you're tracking context or looking for cross-file interactions.]\n",
    "</think>\n",
    "<tool>\n",
    "JSON with the following fields:\n",
    "- name: The name of the tool to call\n",
    "- args: A dictionary of arguments to pass to the tool (must be valid JSON)\n",
    "</tool>\n",
    "\n",
    "When you are done, provide a clear and structured security review in the following format:\n",
    "\n",
    "<answer>\n",
    "Name of the file being analyzed: $FILE_NAME\n",
    "1. Summary of Code Purpose\n",
    "2. Detected Vulnerabilities (with line numbers and severity) and explanation of each issue (why it's dangerous, relevant CVE/CWE/OWASP ref). Explicitly mention if a vulnerability spans multiple files, describing the interaction.\n",
    "3. Suggested Fixes (with example code and links if possible and helpful)\n",
    "---------------------------------\n",
    "</answer>\n",
    "\n",
    "The answer should be well-structured and easily readable.\n",
    "If analyzing multiple files (i.e., a directory was provided as input), first generate a \"summary\" report. This summary should highlight:\n",
    "  - The overall purpose of the project/directory.\n",
    "  - The most critical vulnerabilities found, especially those involving interactions between multiple files (e.g., \"Command injection in `app.py` via `utils.py:execute_system_command` due to data flow from user input\").\n",
    "  - General recommendations or patterns observed.\n",
    "Then, provide a detailed report for each individual file analyzed. All reports (summary and per-file) should be in one answer block, separated by \"---------------------------------\".\n",
    "Use the name \"summary\" for the summary section. If only a single file was analyzed, omit the summary block and directly provide the report for that file.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7f28c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(path: Union[str, os.PathLike]) -> list[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Recursively loads all .py files from a file or directory.\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: [(file_path, file_content), ...]\n",
    "    \"\"\"\n",
    "    files = []\n",
    "\n",
    "    if os.path.isfile(path) and path.endswith(\".py\"):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            files.append((path, f.read()))\n",
    "    elif os.path.isdir(path):\n",
    "        for root, _, filenames in os.walk(path):\n",
    "            for filename in filenames:\n",
    "                if filename.endswith(\".py\"):\n",
    "                    full_path = os.path.join(root, filename)\n",
    "                    try:\n",
    "                        with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                            files.append((full_path, f.read()))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Could not read {full_path}: {e}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Path must be a .py file or a directory: {path}\")\n",
    "\n",
    "    return files\n",
    "\n",
    "def static_analysis(code: str) -> list:\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".py\", mode='w+', delete=False) as tmp:\n",
    "        tmp.write(code)\n",
    "        tmp.flush()\n",
    "        result = subprocess.run(\n",
    "            [\"bandit\", \"-f\", \"json\", tmp.name],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        try:\n",
    "            output = json.loads(result.stdout)\n",
    "            return [\n",
    "                {\n",
    "                    \"line\": item[\"line_number\"],\n",
    "                    \"issue\": item[\"issue_text\"],\n",
    "                    \"severity\": item[\"issue_severity\"],\n",
    "                    \"confidence\": item[\"issue_confidence\"],\n",
    "                    \"id\": item[\"test_id\"]\n",
    "                }\n",
    "                for item in output.get(\"results\", [])\n",
    "            ]\n",
    "        except Exception as e:\n",
    "            return [{\"error\": str(e)}]\n",
    "\n",
    "def parse_ast(code: str) -> dict:\n",
    "    tree = ast.parse(code)\n",
    "    functions = []\n",
    "    risky_calls = []\n",
    "    imports = []\n",
    "\n",
    "    class Analyzer(ast.NodeVisitor):\n",
    "        def visit_FunctionDef(self, node):\n",
    "            functions.append(node.name)\n",
    "            self.generic_visit(node)\n",
    "\n",
    "        def visit_Call(self, node):\n",
    "            if isinstance(node.func, ast.Attribute):\n",
    "                func_name = f\"{ast.unparse(node.func.value)}.{node.func.attr}\"\n",
    "                if func_name in [\"os.system\", \"eval\", \"exec\", \"pickle.load\", \"subprocess.Popen\"]:\n",
    "                    risky_calls.append({\n",
    "                        \"line\": node.lineno,\n",
    "                        \"call\": func_name,\n",
    "                        \"arg\": ast.unparse(node.args[0]) if node.args else \"\"\n",
    "                    })\n",
    "            self.generic_visit(node)\n",
    "\n",
    "        def visit_Import(self, node):\n",
    "            for alias in node.names:\n",
    "                imports.append(alias.name)\n",
    "\n",
    "        def visit_ImportFrom(self, node):\n",
    "            imports.append(node.module)\n",
    "\n",
    "    Analyzer().visit(tree)\n",
    "\n",
    "    return {\n",
    "        \"functions\": functions,\n",
    "        \"risky_calls\": risky_calls,\n",
    "        \"imports\": imports\n",
    "    }\n",
    "    \n",
    "def brave_search(query: str) -> list[str]:\n",
    "    url = \"https://api.search.brave.com/res/v1/web/search\"\n",
    "    headers = {\"Accept\": \"application/json\", \"X-Subscription-Token\": BRAVE_API_KEY}\n",
    "    params = {\"q\": query, \"count\": 5, \"freshness\": \"Month\"}\n",
    "\n",
    "    resp = requests.get(url, headers=headers, params=params)\n",
    "    data = resp.json()\n",
    "    results = data.get(\"web\", {}).get(\"results\", [])\n",
    "    return [r.get(\"title\", \"\") + \"\\n\" + r.get(\"description\", \"\") for r in results]\n",
    "\n",
    "def doc_search_with_brave(query: str, model: str = \"gpt-4.1\") -> str:\n",
    "    results = brave_search(query)\n",
    "    context = \"\\n\\n\".join(results)\n",
    "\n",
    "    prompt = (\n",
    "        \"You are a security expert. Answer the following question using the information \"\n",
    "        \"from recent search results:\\n\\n\"\n",
    "        f\"Search results:\\n{context}\\n\\n\"\n",
    "        f\"Question: {query}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def suggest_fix(issue: str, code_snippet: str, model_name: str = \"gpt-4.1\") -> str:\n",
    "    prompt = (\n",
    "        \"You are a secure code advisor.\\n\"\n",
    "        f\"The following code has a security issue: {issue}.\\n\"\n",
    "        \"Suggest a safer version of the code and explain why it's better.\\n\\n\"\n",
    "        f\"Code:\\n{code_snippet}\"\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81206b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_thinking_from_response(response: str) -> Optional[str]:\n",
    "    \"\"\"Extract the <think> block from the LLM response.\"\"\"\n",
    "    match = re.search(r\"<think>(.*?)</think>\", response, re.DOTALL)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "\n",
    "def parse_tool_from_response(response: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Extract the <tool> call as a dictionary from the LLM response.\"\"\"\n",
    "    match = re.search(r\"<tool>(.*?)</tool>\", response, re.DOTALL)\n",
    "    if not match:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(match.group(1))\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON parsing error in <tool>: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_answer_from_response(response: str) -> Optional[str]:\n",
    "    \"\"\"Extract the <answer> block from the LLM response.\"\"\"\n",
    "    match = re.search(r\"<answer>(.*?)</answer>\", response, re.DOTALL)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "\n",
    "def sanitize_filename(name: str) -> str:\n",
    "    \"\"\"Sanitize the file name for pretty and safe filesystem usage.\n",
    "    Generates names like 'SecureSage-Report-your-file-name.md' (extension added later).\n",
    "    \"\"\"\n",
    "    \n",
    "    prefix = \"SecureSage-Report-\"\n",
    "    \n",
    "    # Handle special \"summary\" case (case-insensitive)\n",
    "    if name.lower() == \"summary\":\n",
    "        base_name_intermediate = \"summary\"\n",
    "    else:\n",
    "        processed_name = name\n",
    "        while processed_name.startswith((\"./\", \".\\\\\")):\n",
    "            processed_name = processed_name[2:]\n",
    "        while processed_name.startswith((\"../\", \"..\\\\\")):\n",
    "            processed_name = processed_name[3:]\n",
    "            \n",
    "        # Replace directory separators with hyphens\n",
    "        base_name_intermediate = processed_name.replace(\"/\", \"-\").replace(\"\\\\\", \"-\")\n",
    "        \n",
    "        if base_name_intermediate.lower().endswith(\".py\"):\n",
    "            base_name_intermediate = base_name_intermediate[:-3]\n",
    "            \n",
    "    base_name_slug = base_name_intermediate.lower()\n",
    "    base_name_slug = re.sub(r\"[^a-z0-9\\-]+\", \"-\", base_name_slug)\n",
    "    base_name_slug = re.sub(r\"-+\", \"-\", base_name_slug)\n",
    "    base_name_slug = base_name_slug.strip(\"-\")\n",
    "\n",
    "    if not base_name_slug:\n",
    "        base_name_slug = \"untitled-report\"\n",
    "        \n",
    "    full_sanitized_name = f\"{prefix}{base_name_slug}\"\n",
    "    return full_sanitized_name[:250]\n",
    "\n",
    "def split_and_write_answers(raw_answer: str, output_dir: str = \"reports\") -> None:\n",
    "    \"\"\"\n",
    "    Splits a SecureSage response block into individual sections per file and writes each to its own .md file.\n",
    "    Sections must start with:   Name of the file being analyzed: <filename>\n",
    "    and end with:               ---------------------------------\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Split at each delimiter\n",
    "    sections = raw_answer.strip().split('---------------------------------')\n",
    "\n",
    "    for section in sections:\n",
    "        # Find the file name\n",
    "        file_match = re.search(r\"Name of the file being analyzed:\\s*(.+)\", section)\n",
    "        if not file_match:\n",
    "            continue  # skip malformed blocks\n",
    "\n",
    "        file_name = file_match.group(1).strip()\n",
    "        safe_name = sanitize_filename(file_name)\n",
    "        file_path = os.path.join(output_dir, f\"{safe_name}.md\")\n",
    "\n",
    "        # Write the rest of the section to file\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"# SecureSage Security Report\\n\\n\")\n",
    "            f.write(section.strip())\n",
    "            f.write(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0bb4f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== AGENT TURN 0 ================\n",
      "\n",
      "\n",
      "Agent thought:\n",
      "I need to start by loading all the files in the ./example_code/test_project/ directory, so I can get an overview of the files, their contents, and their interdependencies. This will allow me to track sources and sinks of data, spot sensitive logic, and plan deeper analysis for potential vulnerabilities or risky interactions between files.\n",
      "\n",
      "Tool call:\n",
      "Tool: load_files\n",
      "Args: {\n",
      "  \"path\": \"./example_code/test_project/\"\n",
      "}\n",
      "\n",
      "Tool result:\n",
      "[\n",
      "  [\n",
      "    \"./example_code/test_project/config.py\",\n",
      "    \"API_KEY = \\\"hardcoded_super_secret_key_12345\\\"\\n\\nDEFAULT_USERNAME = \\\"admin\\\"\\n\\ndef get_database_url():\\n    return f\\\"mysql://{DEFAULT_USERNAME}:weakpassword@localhost/mydatabase\\\"\"\n",
      "  ],\n",
      "  [\n",
      "    \"./example_code/test_project/utils.py\",\n",
      "    \"# ./test_project/utils.py\\nimport subprocess\\nimport pickle\\nimport base64\\n\\ndef execute_system_command(command_string):\\n    # Vulnerability: Command Injection if command_string is user-controlled\\n    # The 'shell=True' is particularly dangerous.\\n    try:\\n        result = subprocess.run(command_string, shell=True, check=True, capture_output=True, text=True)\\n        return result.stdout\\n    except subprocess.CalledProcessError as e:\\n        return f\\\"Error: {e.stderr}\\\"\\n\\ndef unsafe_deserialize(base64_encoded_data):\\n    # Vulnerability: Insecure Deserialization\\n    try:\\n        pickled_data = base64.b64decode(base64_encoded_data)\\n        return pickle.loads(pickled_data)\\n    except Exception as e:\\n        return f\\\"Deserialization error: {e}\\\"\"\n",
      "  ],\n",
      "  [\n",
      "    \"./example_code/test_project/app.py\",\n",
      "    \"# ./test_project/app.py\\nfrom config import API_KEY, get_database_url\\nfrom utils import execute_system_command, unsafe_deserialize\\nimport os # For pickle payload example\\n\\ndef initialize_app():\\n    print(f\\\"Initializing with API Key: {API_KEY}\\\") # Use of hardcoded secret from config.py\\n    db_url = get_database_url()\\n    print(f\\\"Connecting to DB: {db_url}\\\") # Potential info leak if URL contains sensitive data\\n\\ndef handle_user_request(user_data):\\n    # Simulating user input that might be part of a filename or path\\n    # This demonstrates data flow from app.py to utils.py for command execution\\n    result = execute_system_command(f\\\"ls -l {user_data}\\\") # Command Injection via utils.execute_system_command\\n    print(f\\\"Command result: {result}\\\")\\n\\ndef process_serialized_object(encoded_object_string):\\n    # Data flow to insecure deserialization in utils.py\\n    obj = unsafe_deserialize(encoded_object_string)\\n    print(f\\\"Processed object: {obj}\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    initialize_app()\\n    \\n    # Simulate potentially malicious user input for command injection\\n    malicious_path_input = \\\"nonexistent_file; echo 'PWNED!'\\\"\\n    handle_user_request(malicious_path_input)\\n\\n    # Simulate receiving a malicious base64 encoded pickle string\\n    # This payload executes `os.system(\\\"echo 'Pickle RCE'\\\")`\\n    # To generate: import pickle, base64, os; payload = base64.b64encode(pickle.dumps(os.system)).decode() + base64.b64encode(pickle.dumps((\\\"echo 'Pickle RCE'\\\",))).decode() # simplified\\n    # A simpler malicious pickle: base64.b64encode(pickle.dumps(os.system(\\\"touch /tmp/pwned_by_pickle\\\")))\\n    # For this example: `pickle.dumps(__import__('os').system('echo vulnerable_pickle'))` then base64 encoded\\n    malicious_pickle_b64 = b\\\"gASVHgAAAAAAAACMCF9faW1wb3J0X19lWAMAAABvc5SUTlgEAAAAZWNobyB2dWxuZXJhYmxlX3BpY2tlk5RSlC4=\\\"\\n    process_serialized_object(malicious_pickle_b64)\"\n",
      "  ]\n",
      "]\n",
      "=============== AGENT TURN 1 ================\n",
      "\n",
      "\n",
      "Agent thought:\n",
      "I've loaded the files: config.py, utils.py, and app.py. Based on their contents, I see several potential vulnerabilities:\n",
      "- Hardcoded secrets and weak credentials in config.py.\n",
      "- Dangerous use of subprocess with shell=True and insecure deserialization with pickle in utils.py.\n",
      "- app.py demonstrates data flows from user input directly into these dangerous sinks across file boundaries.\n",
      "\n",
      "Next, I'll run static_analysis on app.py, as it serves as the orchestrator and demonstrates real data flows between user input and the dangerous functions in utils.py. This step will help flag line numbers, severity, and types of vulnerabilities.\n",
      "\n",
      "Tool call:\n",
      "Tool: static_analysis\n",
      "Args: {\n",
      "  \"code\": \"# ./test_project/app.py\\nfrom config import API_KEY, get_database_url\\nfrom utils import execute_system_command, unsafe_deserialize\\nimport os # For pickle payload example\\n\\ndef initialize_app():\\n    print(f\\\"Initializing with API Key: {API_KEY}\\\") # Use of hardcoded secret from config.py\\n    db_url = get_database_url()\\n    print(f\\\"Connecting to DB: {db_url}\\\") # Potential info leak if URL contains sensitive data\\n\\ndef handle_user_request(user_data):\\n    # Simulating user input that might be part of a filename or path\\n    # This demonstrates data flow from app.py to utils.py for command execution\\n    result = execute_system_command(f\\\"ls -l {user_data}\\\") # Command Injection via utils.execute_system_command\\n    print(f\\\"Command result: {result}\\\")\\n\\ndef process_serialized_object(encoded_object_string):\\n    # Data flow to insecure deserialization in utils.py\\n    obj = unsafe_deserialize(encoded_object_string)\\n    print(f\\\"Processed object: {obj}\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    initialize_app()\\n    \\n    # Simulate potentially malicious user input for command injection\\n    malicious_path_input = \\\"nonexistent_file; echo 'PWNED!'\\\"\\n    handle_user_request(malicious_path_input)\\n\\n    # Simulate receiving a malicious base64 encoded pickle string\\n    # This payload executes `os.system(\\\"echo 'Pickle RCE'\\\")`\\n    # To generate: import pickle, base64, os; payload = base64.b64encode(pickle.dumps(os.system)).decode() + base64.b64encode(pickle.dumps((\\\"echo 'Pickle RCE'\\\",))).decode() # simplified\\n    # A simpler malicious pickle: base64.b64encode(pickle.dumps(os.system(\\\"touch /tmp/pwned_by_pickle\\\")))\\n    # For this example: `pickle.dumps(__import__('os').system('echo vulnerable_pickle'))` then base64 encoded\\n    malicious_pickle_b64 = b\\\"gASVHgAAAAAAAACMCF9faW1wb3J0X19lWAMAAABvc5SUTlgEAAAAZWNobyB2dWxuZXJhYmxlX3BpY2tlk5RSlC4=\\\"\\n    process_serialized_object(malicious_pickle_b64)\"\n",
      "}\n",
      "\n",
      "Tool result:\n",
      "[]\n",
      "=============== AGENT TURN 2 ================\n",
      "\n",
      "\n",
      "Agent thought:\n",
      "The static analysis on app.py did not return explicit findings, likely because the critical vulnerabilities reside in the imported functions (from utils.py), not directly in this file. However, app.py orchestrates user input and shows data flows that could enable exploits.\n",
      "\n",
      "Next, I will analyze utils.py with static_analysis, since it contains both the execute_system_command and unsafe_deserialize functions, which are evidently dangerous and receive user-influenced values from app.py. This should yield explicit vulnerability detections with line numbers and severity.\n",
      "\n",
      "Tool call:\n",
      "Tool: static_analysis\n",
      "Args: {\n",
      "  \"code\": \"# ./test_project/utils.py\\nimport subprocess\\nimport pickle\\nimport base64\\n\\ndef execute_system_command(command_string):\\n    # Vulnerability: Command Injection if command_string is user-controlled\\n    # The 'shell=True' is particularly dangerous.\\n    try:\\n        result = subprocess.run(command_string, shell=True, check=True, capture_output=True, text=True)\\n        return result.stdout\\n    except subprocess.CalledProcessError as e:\\n        return f\\\"Error: {e.stderr}\\\"\\n\\ndef unsafe_deserialize(base64_encoded_data):\\n    # Vulnerability: Insecure Deserialization\\n    try:\\n        pickled_data = base64.b64decode(base64_encoded_data)\\n        return pickle.loads(pickled_data)\\n    except Exception as e:\\n        return f\\\"Deserialization error: {e}\\\"\"\n",
      "}\n",
      "\n",
      "Tool result:\n",
      "[\n",
      "  {\n",
      "    \"line\": 2,\n",
      "    \"issue\": \"Consider possible security implications associated with the subprocess module.\",\n",
      "    \"severity\": \"LOW\",\n",
      "    \"confidence\": \"HIGH\",\n",
      "    \"id\": \"B404\"\n",
      "  },\n",
      "  {\n",
      "    \"line\": 3,\n",
      "    \"issue\": \"Consider possible security implications associated with pickle module.\",\n",
      "    \"severity\": \"LOW\",\n",
      "    \"confidence\": \"HIGH\",\n",
      "    \"id\": \"B403\"\n",
      "  },\n",
      "  {\n",
      "    \"line\": 10,\n",
      "    \"issue\": \"subprocess call with shell=True identified, security issue.\",\n",
      "    \"severity\": \"HIGH\",\n",
      "    \"confidence\": \"HIGH\",\n",
      "    \"id\": \"B602\"\n",
      "  },\n",
      "  {\n",
      "    \"line\": 19,\n",
      "    \"issue\": \"Pickle and modules that wrap it can be unsafe when used to deserialize untrusted data, possible security issue.\",\n",
      "    \"severity\": \"MEDIUM\",\n",
      "    \"confidence\": \"HIGH\",\n",
      "    \"id\": \"B301\"\n",
      "  }\n",
      "]\n",
      "=============== AGENT TURN 3 ================\n",
      "\n",
      "\n",
      "Agent thought:\n",
      "The static analysis confirms:\n",
      "- High severity: subprocess with shell=True (line 10), exposing the project to command injection.\n",
      "- Medium severity: Insecure use of pickle for deserialization (line 19), which can enable remote code execution (RCE) if an attacker supplies the payload.\n",
      "There are also advisories about importing subprocess and pickle, but the key risks are the actual function calls.\n",
      "\n",
      "Now, I should analyze config.py. Although it's configuration, it can harbor hardcoded secrets, weak passwords, or information disclosure. I'll run static_analysis on config.py to confirm and help pinpoint line numbers.\n",
      "\n",
      "Tool call:\n",
      "Tool: static_analysis\n",
      "Args: {\n",
      "  \"code\": \"API_KEY = \\\"hardcoded_super_secret_key_12345\\\"\\n\\nDEFAULT_USERNAME = \\\"admin\\\"\\n\\ndef get_database_url():\\n    return f\\\"mysql://{DEFAULT_USERNAME}:weakpassword@localhost/mydatabase\\\"\"\n",
      "}\n",
      "\n",
      "Tool result:\n",
      "[]\n",
      "=============== AGENT TURN 4 ================\n",
      "\n",
      "\n",
      "Agent thought:\n",
      "Static analysis on config.py didn’t flag issues, likely because tools like Bandit don’t always report hardcoded secrets or weak credentials unless specific patterns are triggered. However, I can see from inspection that credentials and secrets are hardcoded directly into the file.\n",
      "\n",
      "To supplement, I'll use parse_ast on config.py to confirm which variables and functions handle sensitive data or secrets, and whether they're used elsewhere insecurely.\n",
      "\n",
      "Tool call:\n",
      "Tool: parse_ast\n",
      "Args: {\n",
      "  \"code\": \"API_KEY = \\\"hardcoded_super_secret_key_12345\\\"\\n\\nDEFAULT_USERNAME = \\\"admin\\\"\\n\\ndef get_database_url():\\n    return f\\\"mysql://{DEFAULT_USERNAME}:weakpassword@localhost/mydatabase\\\"\"\n",
      "}\n",
      "\n",
      "Tool result:\n",
      "{\n",
      "  \"functions\": [\n",
      "    \"get_database_url\"\n",
      "  ],\n",
      "  \"risky_calls\": [],\n",
      "  \"imports\": []\n",
      "}\n",
      "=============== AGENT TURN 5 ================\n",
      "\n",
      "\n",
      "Agent thought:\n",
      "AST parsing confirms that config.py defines one function, get_database_url(), and two hardcoded variables: API_KEY and DEFAULT_USERNAME. The function itself returns a database URL with a hardcoded weak password, representing a credential exposure risk.\n",
      "\n",
      "I’ve now sufficiently analyzed the files and identified:\n",
      "- Command injection and insecure deserialization vulnerabilities due to how app.py interacts with utils.py.\n",
      "- Hardcoded secrets and weak credentials in config.py.\n",
      "I can now provide a structured and detailed report: a summary for the project and per-file reports focusing on the most critical findings, including how the vulnerabilities span across files.\n",
      "ANSWER GIVEN BY AGENT!\n",
      "Report written to markdown.\n"
     ]
    }
   ],
   "source": [
    "# Agent memory\n",
    "messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "tool_call_count = 0\n",
    "max_turns = 10\n",
    "\n",
    "tool_registry = {\n",
    "    \"load_files\": load_files,\n",
    "    \"static_analysis\": static_analysis,\n",
    "    \"parse_ast\": parse_ast,\n",
    "    \"doc_search_with_brave\": doc_search_with_brave,\n",
    "    \"suggest_fix\": suggest_fix,\n",
    "}\n",
    "\n",
    "\n",
    "user_input = f\"Please analyze {CODE_PATH} for vulnerabilities.\"\n",
    "messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "file_name = os.path.basename(CODE_PATH).replace(\".py\", \"\")\n",
    "\n",
    "while tool_call_count < max_turns:\n",
    "    print(f\"=============== AGENT TURN {tool_call_count} ================\\n\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    reply = response.choices[0].message.content\n",
    "    messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "\n",
    "    thought = parse_thinking_from_response(reply)\n",
    "    if thought:\n",
    "        print(\"\\nAgent thought:\")\n",
    "        print(thought)\n",
    "\n",
    "    answer = parse_answer_from_response(reply)\n",
    "    if answer:\n",
    "        print(\"ANSWER GIVEN BY AGENT!\")\n",
    "\n",
    "        split_and_write_answers(answer)\n",
    "        print(\"Report written to markdown.\")\n",
    "        break\n",
    "\n",
    "    tool_call = parse_tool_from_response(reply)\n",
    "    if not tool_call:\n",
    "        print(\"No tool call found. Exiting.\")\n",
    "        break\n",
    "\n",
    "    tool_name = tool_call[\"name\"]\n",
    "    args = tool_call[\"args\"]\n",
    "\n",
    "    print(\"\\nTool call:\")\n",
    "    print(f\"Tool: {tool_name}\")\n",
    "    print(f\"Args: {json.dumps(args, indent=2)}\")\n",
    "\n",
    "    tool_func = tool_registry.get(tool_name)\n",
    "    if not tool_func:\n",
    "        print(f\"Unknown tool: {tool_name}\")\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        result = tool_func(**args)\n",
    "    except Exception as e:\n",
    "        result = {\"error\": str(e)}\n",
    "\n",
    "    print(\"\\nTool result:\")\n",
    "    print(json.dumps(result, indent=2))\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": json.dumps(result, indent=2)})\n",
    "    tool_call_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2195137",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
