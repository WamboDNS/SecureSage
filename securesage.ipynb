{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "612c90e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_PATH = \"./test/code2.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e1f06b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import Optional, Dict, Any\n",
    "from openai import OpenAI\n",
    "import tempfile\n",
    "import subprocess\n",
    "import json\n",
    "import ast\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable is not set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7dd6140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4.1\"\n",
    "base_URL = \"https://api.openai.com/v1\"\n",
    "client = OpenAI(api_key=API_KEY, base_url=base_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4655cc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are SecureSage â€” a vigilant, intelligent, and explainable security analyst. Your task is to review Python source code files, understand what the code is doing, and identify potential security vulnerabilities, such as insecure deserialization, command injection, hardcoded secrets, and other OWASP Top 10 issues.\n",
    "\n",
    "You do this by performing step-by-step analysis. You are allowed to use the following tools:\n",
    "\n",
    "- load_code(path: str) -> str: Loads and returns the contents of a Python (.py) source code file.\n",
    "- static_analysis(code: str) -> str: Runs static security scanners (e.g., Bandit) and returns a list of flagged lines with issue types and severity.\n",
    "- parse_ast(code: str) -> str: Parses the code into an abstract syntax tree and extracts function names, inputs, and risky constructs (e.g., eval, exec, os.system).\n",
    "- doc_search(query: str) -> str: Searches security documentation (e.g., OWASP, CWE, Python docs) for best practices or known issues.\n",
    "- suggest_fix(issue: str, code_snippet: str) -> str: Proposes a secure version of the code snippet that mitigates the vulnerability.\n",
    "\n",
    "You may call one tool per turn, for up to 10 turns, before giving your final answer.\n",
    "\n",
    "In each turn, respond in the following format:\n",
    "\n",
    "<think>\n",
    "[Explain what you're doing next, what you need, or what issue you're focusing on.]\n",
    "</think>\n",
    "<tool>\n",
    "JSON with the following fields:\n",
    "- name: The name of the tool to call\n",
    "- args: A dictionary of arguments to pass to the tool (must be valid JSON)\n",
    "</tool>\n",
    "\n",
    "When you are done, provide a clear and structured security review in the following format:\n",
    "\n",
    "<answer>\n",
    "1. Summary of Code Purpose  \n",
    "2. Detected Vulnerabilities (with line numbers and severity) and explanation of Each Issue (why it's dangerous, relevant CVE/CWE/OWASP ref) \n",
    "3. Suggested Fixes (with example code and links if needed)  \n",
    "</answer>\n",
    "\n",
    "The answer should be nice and structured, and very readable.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7f28c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_code(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def static_analysis(code: str) -> list:\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".py\", mode='w+', delete=False) as tmp:\n",
    "        tmp.write(code)\n",
    "        tmp.flush()\n",
    "        result = subprocess.run(\n",
    "            [\"bandit\", \"-f\", \"json\", tmp.name],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        try:\n",
    "            output = json.loads(result.stdout)\n",
    "            return [\n",
    "                {\n",
    "                    \"line\": item[\"line_number\"],\n",
    "                    \"issue\": item[\"issue_text\"],\n",
    "                    \"severity\": item[\"issue_severity\"],\n",
    "                    \"confidence\": item[\"issue_confidence\"],\n",
    "                    \"id\": item[\"test_id\"]\n",
    "                }\n",
    "                for item in output.get(\"results\", [])\n",
    "            ]\n",
    "        except Exception as e:\n",
    "            return [{\"error\": str(e)}]\n",
    "\n",
    "def parse_ast(code: str) -> dict:\n",
    "    tree = ast.parse(code)\n",
    "    functions = []\n",
    "    risky_calls = []\n",
    "    imports = []\n",
    "\n",
    "    class Analyzer(ast.NodeVisitor):\n",
    "        def visit_FunctionDef(self, node):\n",
    "            functions.append(node.name)\n",
    "            self.generic_visit(node)\n",
    "\n",
    "        def visit_Call(self, node):\n",
    "            if isinstance(node.func, ast.Attribute):\n",
    "                func_name = f\"{ast.unparse(node.func.value)}.{node.func.attr}\"\n",
    "                if func_name in [\"os.system\", \"eval\", \"exec\", \"pickle.load\", \"subprocess.Popen\"]:\n",
    "                    risky_calls.append({\n",
    "                        \"line\": node.lineno,\n",
    "                        \"call\": func_name,\n",
    "                        \"arg\": ast.unparse(node.args[0]) if node.args else \"\"\n",
    "                    })\n",
    "            self.generic_visit(node)\n",
    "\n",
    "        def visit_Import(self, node):\n",
    "            for alias in node.names:\n",
    "                imports.append(alias.name)\n",
    "\n",
    "        def visit_ImportFrom(self, node):\n",
    "            imports.append(node.module)\n",
    "\n",
    "    Analyzer().visit(tree)\n",
    "\n",
    "    return {\n",
    "        \"functions\": functions,\n",
    "        \"risky_calls\": risky_calls,\n",
    "        \"imports\": imports\n",
    "    }\n",
    "    \n",
    "doc_database = {\n",
    "    \"os.system\": \"Avoid using os.system with user input. Prefer subprocess.run([...], check=True).\",\n",
    "    \"eval\": \"Eval executes strings as code. Dangerous if input-controlled.\",\n",
    "    \"hardcoded secret\": \"Never commit secrets. Use environment variables or a vault.\"\n",
    "}\n",
    "\n",
    "def doc_search(query: str) -> str:\n",
    "    for keyword, advice in doc_database.items():\n",
    "        if keyword in query.lower():\n",
    "            return advice\n",
    "    return \"No documentation match found for your query.\"\n",
    "\n",
    "\n",
    "def suggest_fix(issue: str, code_snippet: str, model_name: str = \"gpt-4.1\") -> str:\n",
    "    prompt = (\n",
    "        \"You are a secure code advisor.\\n\"\n",
    "        f\"The following code has a security issue: {issue}.\\n\"\n",
    "        \"Suggest a safer version of the code and explain why it's better.\\n\\n\"\n",
    "        f\"Code:\\n{code_snippet}\"\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81206b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_thinking_from_response(response: str) -> Optional[str]:\n",
    "    \"\"\"Extract the <think> block from the LLM response.\"\"\"\n",
    "    match = re.search(r\"<think>(.*?)</think>\", response, re.DOTALL)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "\n",
    "def parse_tool_from_response(response: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Extract the <tool> call as a dictionary from the LLM response.\"\"\"\n",
    "    match = re.search(r\"<tool>(.*?)</tool>\", response, re.DOTALL)\n",
    "    if not match:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(match.group(1))\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON parsing error in <tool>: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_answer_from_response(response: str) -> Optional[str]:\n",
    "    \"\"\"Extract the <answer> block from the LLM response.\"\"\"\n",
    "    match = re.search(r\"<answer>(.*?)</answer>\", response, re.DOTALL)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "\n",
    "def write_answer_to_markdown(answer: str, file_path: str = \"reports/secure_sage_report.md\") -> None:\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)  # Ensure folder exists\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"# SecureSage Security Report\\n\\n\")\n",
    "        f.write(answer.strip())\n",
    "        f.write(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0bb4f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== AGENT TURN 0 ================\n",
      "\n",
      "\n",
      "Agent thought:\n",
      "First, I need to load the contents of ./test/code2.py to understand what the code does and to prepare for analysis.\n",
      "\n",
      "Tool call:\n",
      "Tool: load_code\n",
      "Args: {\n",
      "  \"path\": \"./test/code2.py\"\n",
      "}\n",
      "\n",
      "Tool result:\n",
      "\"import os\\nimport json\\nimport secrets\\nfrom flask import Flask, request\\n\\napp = Flask(__name__)\\napp.config['DEBUG'] = True\\n\\nwith open(\\\"config.json\\\") as f:\\n    config = json.load(f)\\n\\nAPI_KEY = config.get(\\\"api_key\\\", \\\"default_key\\\")\\n\\n@app.route(\\\"/generate\\\", methods=[\\\"GET\\\"])\\ndef generate_token():\\n    user_id = request.args.get(\\\"user\\\")\\n    token = str(secrets.randbelow(1000000))\\n    return f\\\"Token for {user_id}: {token}\\\"\\n\\n@app.route(\\\"/execute\\\", methods=[\\\"POST\\\"])\\ndef execute_command():\\n    data = request.get_json()\\n    cmd = data.get(\\\"cmd\\\")\\n    os.system(cmd)\\n    return \\\"Command executed.\\\"\\n\\n@app.route(\\\"/data\\\", methods=[\\\"POST\\\"])\\ndef upload_data():\\n    payload = request.data\\n    data = json.loads(payload)\\n    return f\\\"Received: {data}\\\"\\n\"\n",
      "=============== AGENT TURN 1 ================\n",
      "\n",
      "\n",
      "Agent thought:\n",
      "Now that I've loaded the code, I'll run a static analysis. This will quickly flag any common security issues, identify risky lines (e.g., use of os.system, json.loads, etc.), and help focus my manual review.\n",
      "\n",
      "Tool call:\n",
      "Tool: static_analysis\n",
      "Args: {\n",
      "  \"code\": \"import os\\nimport json\\nimport secrets\\nfrom flask import Flask, request\\n\\napp = Flask(__name__)\\napp.config['DEBUG'] = True\\n\\nwith open(\\\"config.json\\\") as f:\\n    config = json.load(f)\\n\\nAPI_KEY = config.get(\\\"api_key\\\", \\\"default_key\\\")\\n\\n@app.route(\\\"/generate\\\", methods=[\\\"GET\\\"])\\ndef generate_token():\\n    user_id = request.args.get(\\\"user\\\")\\n    token = str(secrets.randbelow(1000000))\\n    return f\\\"Token for {user_id}: {token}\\\"\\n\\n@app.route(\\\"/execute\\\", methods=[\\\"POST\\\"])\\ndef execute_command():\\n    data = request.get_json()\\n    cmd = data.get(\\\"cmd\\\")\\n    os.system(cmd)\\n    return \\\"Command executed.\\\"\\n\\n@app.route(\\\"/data\\\", methods=[\\\"POST\\\"])\\ndef upload_data():\\n    payload = request.data\\n    data = json.loads(payload)\\n    return f\\\"Received: {data}\\\"\\n\"\n",
      "}\n",
      "\n",
      "Tool result:\n",
      "[\n",
      "  {\n",
      "    \"line\": 24,\n",
      "    \"issue\": \"Starting a process with a shell, possible injection detected, security issue.\",\n",
      "    \"severity\": \"HIGH\",\n",
      "    \"confidence\": \"HIGH\",\n",
      "    \"id\": \"B605\"\n",
      "  }\n",
      "]\n",
      "=============== AGENT TURN 2 ================\n",
      "\n",
      "\n",
      "Agent thought:\n",
      "The static analysis flagged use of os.system (line 24) as a critical command injection risk. I should confirm how user input gets to this line and look for additional issues manually: the application receives a POST JSON payload, takes the \"cmd\" field, and executes it via os.system with no sanitization.\n",
      "\n",
      "There's also potential for other vulnerabilities (e.g., unsafe JSON deserialization, debug mode enabled, possible hardcoded/default credentials). I'll now parse the code's AST to quickly enumerate function names, user inputs, and other risky constructs.\n",
      "\n",
      "Tool call:\n",
      "Tool: parse_ast\n",
      "Args: {\n",
      "  \"code\": \"import os\\nimport json\\nimport secrets\\nfrom flask import Flask, request\\n\\napp = Flask(__name__)\\napp.config['DEBUG'] = True\\n\\nwith open(\\\"config.json\\\") as f:\\n    config = json.load(f)\\n\\nAPI_KEY = config.get(\\\"api_key\\\", \\\"default_key\\\")\\n\\n@app.route(\\\"/generate\\\", methods=[\\\"GET\\\"])\\ndef generate_token():\\n    user_id = request.args.get(\\\"user\\\")\\n    token = str(secrets.randbelow(1000000))\\n    return f\\\"Token for {user_id}: {token}\\\"\\n\\n@app.route(\\\"/execute\\\", methods=[\\\"POST\\\"])\\ndef execute_command():\\n    data = request.get_json()\\n    cmd = data.get(\\\"cmd\\\")\\n    os.system(cmd)\\n    return \\\"Command executed.\\\"\\n\\n@app.route(\\\"/data\\\", methods=[\\\"POST\\\"])\\ndef upload_data():\\n    payload = request.data\\n    data = json.loads(payload)\\n    return f\\\"Received: {data}\\\"\\n\"\n",
      "}\n",
      "\n",
      "Tool result:\n",
      "{\n",
      "  \"functions\": [\n",
      "    \"generate_token\",\n",
      "    \"execute_command\",\n",
      "    \"upload_data\"\n",
      "  ],\n",
      "  \"risky_calls\": [\n",
      "    {\n",
      "      \"line\": 24,\n",
      "      \"call\": \"os.system\",\n",
      "      \"arg\": \"cmd\"\n",
      "    }\n",
      "  ],\n",
      "  \"imports\": [\n",
      "    \"os\",\n",
      "    \"json\",\n",
      "    \"secrets\",\n",
      "    \"flask\"\n",
      "  ]\n",
      "}\n",
      "=============== AGENT TURN 3 ================\n",
      "\n",
      "\n",
      "Agent thought:\n",
      "The AST confirms that user-supplied \"cmd\" POST data goes directly into os.system, a classic and dangerous command injection vector (CWE-78, OWASP A1: Injection). I also notice:\n",
      "- app.config['DEBUG']=True exposes debugging info (do not use in production);\n",
      "- API_KEY uses config.get(..., \"default_key\"), so if config.json is missing or tampered, it uses a weak default API key;\n",
      "- The /data endpoint accepts raw POST data and loads it with json.loads: this is usually safe as JSON, but can lead to DoS/slip-ups if payload is not validated (though not technically insecure deserialization like pickle).\n",
      "\n",
      "I'll now look up best practices for securely handling shell command execution in Python web applications.\n",
      "\n",
      "Tool call:\n",
      "Tool: doc_search\n",
      "Args: {\n",
      "  \"query\": \"Secure alternatives to os.system to prevent command injection in Python web applications\"\n",
      "}\n",
      "\n",
      "Tool result:\n",
      "\"Avoid using os.system with user input. Prefer subprocess.run([...], check=True).\"\n",
      "=============== AGENT TURN 4 ================\n",
      "\n",
      "\n",
      "Agent thought:\n",
      "I now have enough information to provide a structured security review. I will summarize the application's purpose, enumerate all detected vulnerabilities (including command injection, debug mode, and weak default API key), explain why they're dangerous (with references), and suggest practical fixes, citing subprocess.run as a safer alternative for command execution and other best practices.\n",
      "ANSWER GIVEN BY AGENT!\n",
      "Report written to markdown.\n"
     ]
    }
   ],
   "source": [
    "# Agent memory\n",
    "messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "tool_call_count = 0\n",
    "max_turns = 10\n",
    "\n",
    "tool_registry = {\n",
    "    \"load_code\": load_code,\n",
    "    \"static_analysis\": static_analysis,\n",
    "    \"parse_ast\": parse_ast,\n",
    "    \"doc_search\": doc_search,\n",
    "    \"suggest_fix\": suggest_fix,\n",
    "}\n",
    "\n",
    "\n",
    "user_input = f\"Please analyze {CODE_PATH} for vulnerabilities.\"\n",
    "messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "file_name = os.path.basename(CODE_PATH).replace(\".py\", \"\")\n",
    "\n",
    "while tool_call_count < max_turns:\n",
    "    print(f\"=============== AGENT TURN {tool_call_count} ================\\n\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    reply = response.choices[0].message.content\n",
    "    messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "\n",
    "    thought = parse_thinking_from_response(reply)\n",
    "    if thought:\n",
    "        print(\"\\nAgent thought:\")\n",
    "        print(thought)\n",
    "\n",
    "    answer = parse_answer_from_response(reply)\n",
    "    if answer:\n",
    "        print(\"ANSWER GIVEN BY AGENT!\")\n",
    "\n",
    "        write_answer_to_markdown(answer, file_path=f\"./reports/{file_name}_report.md\")\n",
    "        print(\"Report written to markdown.\")\n",
    "        break\n",
    "\n",
    "    tool_call = parse_tool_from_response(reply)\n",
    "    if not tool_call:\n",
    "        print(\"No tool call found. Exiting.\")\n",
    "        break\n",
    "\n",
    "    tool_name = tool_call[\"name\"]\n",
    "    args = tool_call[\"args\"]\n",
    "\n",
    "    print(\"\\nTool call:\")\n",
    "    print(f\"Tool: {tool_name}\")\n",
    "    print(f\"Args: {json.dumps(args, indent=2)}\")\n",
    "\n",
    "    tool_func = tool_registry.get(tool_name)\n",
    "    if not tool_func:\n",
    "        print(f\"Unknown tool: {tool_name}\")\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        result = tool_func(**args)\n",
    "    except Exception as e:\n",
    "        result = {\"error\": str(e)}\n",
    "\n",
    "    print(\"\\nTool result:\")\n",
    "    print(json.dumps(result, indent=2))\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": json.dumps(result, indent=2)})\n",
    "    tool_call_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2195137",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
